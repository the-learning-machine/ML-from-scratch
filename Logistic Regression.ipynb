{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is used to classify data into binary classes. Logistic regression can also be used for multi-class classification. Unlike linear regression, Logistic Regression does not have an explicit form, so the only way to solve it is through gradient ascent.\n",
    "\n",
    "Logistic regression makes predictions by squashing the linear prediction $\\theta^T x$ (also known as a logit, to the $[0,1]$ interval using the sigmoid function \n",
    "\n",
    "$$\\sigma(z) =  1 /(1+e^{-z})$$\n",
    "\n",
    "The output of $\\sigma$ is a probability, so the actual prediction is that if $\\sigma(\\theta^T x) \\geq 0.5$ the sample belongs to one class, otherwise it belongs to the other class. \n",
    "\n",
    "The logistic loss comes from the likelihood of its predictions:\n",
    "\n",
    "$$l = \\prod_i p_i^{y_i} (1-p_i)^{1-y_i}$$\n",
    "\n",
    "where $p_i = \\sigma(\\theta^t x)$. It's mathematically simpler to work with logarithms, and the logistic loss is therefore defined as the log likelihood.\n",
    "\n",
    "$$J \\equiv \\log l =  \\frac 1 n \\sum_i y_i \\log p_i + (1-y_i)  \\log (1-p_i)$$\n",
    "\n",
    "We want to maximize the gradient so we will perform gradient ascent on the parameters $\\theta$. To do that we need to compute the gradient:\n",
    "\n",
    "$$\\nabla_\\theta J = \\nabla_{p} J \\cdot \\nabla_\\theta p$$\n",
    "\n",
    "Since $p = \\sigma(\\theta^T x)$, and $\\sigma' = \\sigma ( 1-\\sigma)$ the gradient of the loss is\n",
    "\n",
    "$$\\nabla_\\theta J = \\frac 1 n \\sum_i x_i^T (y_i - \\sigma(\\theta^T x_i))$$\n",
    "\n",
    "We're performing gradient ascent to maximize the log likelihood. To make it easy to compare to linear regression, let's reformulate this as a gradient descent problem. Then\n",
    "\n",
    "$$\\nabla_\\theta J = \\frac 1 n \\sum_i x_i^T (\\sigma(\\theta^T x_i)-y_i)$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\theta_{k+1} = \\theta_k - \\nabla_\\theta J$$\n",
    "\n",
    "Remarkably, this is the same update rule as we saw in linear regression LMS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Logistic Regression\n",
      "========================================\n",
      "Hyperparamters:\n",
      "lr = 0.01 > learning rate\n",
      "iters = 20 > optimization steps\n",
      "========================================\n",
      "Step 0 Loss -0.208 Score 0.75\n",
      "Step 1 Loss -0.175 Score 0.75\n",
      "Step 2 Loss 0.043 Score 0.9\n",
      "Step 3 Loss 0.085 Score 0.9\n",
      "Step 4 Loss 0.246 Score 1.0\n",
      "Step 5 Loss 0.283 Score 1.0\n",
      "Step 6 Loss 0.314 Score 1.0\n",
      "Step 7 Loss 0.339 Score 1.0\n",
      "Step 8 Loss 0.359 Score 1.0\n",
      "Step 9 Loss 0.375 Score 1.0\n",
      "Step 10 Loss 0.388 Score 1.0\n",
      "Step 11 Loss 0.398 Score 1.0\n",
      "Step 12 Loss 0.406 Score 1.0\n",
      "Step 13 Loss 0.413 Score 1.0\n",
      "Step 14 Loss 0.419 Score 1.0\n",
      "Step 15 Loss 0.424 Score 1.0\n",
      "Step 16 Loss 0.428 Score 1.0\n",
      "Step 17 Loss 0.431 Score 1.0\n",
      "Step 18 Loss 0.494 Score 0.95\n",
      "Step 19 Loss 0.497 Score 0.95\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Procedure LMS\n",
    "1. Normalize data to have mean=0,std=1 for features\n",
    "2. Add interept term to data\n",
    "3. Initialize weights\n",
    "4. For step in n_iters do:\n",
    "5.     predict outputs - pred = sigmoid()\n",
    "6.     compute loss = mean( y log pred + (1-y) log (1-pred)) \n",
    "7.     compute grad_loss = mean((pred-targets)@data) \n",
    "8.     update weights -= lr*grad_loss (maximizes log likelihood)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_data(n,f):\n",
    "    data = np.random.random_sample((n,f))+np.sqrt(np.arange(n*f).reshape(n,f))\n",
    "    targets = np.concatenate((np.zeros(n//2),np.ones(n-n//2)))\n",
    "    return data,targets\n",
    "\n",
    "class LinearRegressionLMS:\n",
    "    \n",
    "    def __init__(self,lr=1e-2,iters=10):\n",
    "        self.lr = lr\n",
    "        self.iters = iters\n",
    "        self.weights = []\n",
    "        \n",
    "    def fit(self,data,targets):\n",
    "        print(self)\n",
    "        n = data.shape[0]\n",
    "        # normalize data + add intercept term\n",
    "        data -= data.mean(0)\n",
    "        data/=(data.std(0)+1e-5)\n",
    "        data = self._add_intercept(data)\n",
    "        f = data.shape[1]\n",
    "        # init and normalize weights\n",
    "        self.weights = np.random.randn(f)\n",
    "        self.weights = (self.weights- self.weights.mean())/(self.weights.std()+1e-5)\n",
    "        for i in range(self.iters):\n",
    "            # predict\n",
    "            pred = self._sigmoid(data @ self.weights)\n",
    "            # compute log probs for loss\n",
    "            logp0 = np.array([np.log(p) for p in pred if p >=.5])\n",
    "            logp1 = np.array([1-p for p in pred if p< 0.5])\n",
    "            # compute loss\n",
    "            loss = np.mean(np.concatenate((logp0,logp1)))\n",
    "            # compute gradient of loss\n",
    "            grad_loss = data.T @ (pred-targets) \n",
    "            # update weights\n",
    "            self.weights -= self.lr * grad_loss\n",
    "            score = self._score(pred,targets)\n",
    "            print('Step',i,'Loss',round(loss,3),'Score',score)\n",
    " \n",
    "    \n",
    "    def _score(self,pred,targets):\n",
    "        out = [0.0 if p>=0.5 else 1.0 for p in pred]\n",
    "        return np.mean([x==y for x,y in zip(out,targets)])\n",
    "\n",
    "    def _sigmoid(self,x):\n",
    "        return 1/(1+np.exp(x))\n",
    "    \n",
    "    def _grad_sigmoid(self,x):\n",
    "        return self._sigmoid(x)*(1-self._sigmoid(x))\n",
    "    \n",
    "    def _add_intercept(self,data):\n",
    "        intercept = np.ones(data.shape[0]).reshape(-1,1)\n",
    "        return np.concatenate((intercept,data),1)\n",
    "    \n",
    "    def __str__(self):\n",
    "        line = '='*40\n",
    "        print(line)\n",
    "        print('Logistic Regression')\n",
    "        print(line)\n",
    "        print('Hyperparamters:')\n",
    "        print('lr =',self.lr,'> learning rate')\n",
    "        print('iters =',self.iters,'> optimization steps')\n",
    "        return line\n",
    "    \n",
    "data, targets = generate_data(20,3)\n",
    "model = LinearRegressionLMS(lr=1e-2,iters=20)\n",
    "model.fit(data,targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
