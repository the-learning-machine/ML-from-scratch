{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is used to classify data into binary classes. Logistic regression can also be used for multi-class classification. Unlike linear regression, Logistic Regression does not have an explicit form, so the only way to solve it is through gradient ascent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Logistic Regression\n",
      "========================================\n",
      "Hyperparamters:\n",
      "lr = 0.01 > learning rate\n",
      "iters = 20 > optimization steps\n",
      "========================================\n",
      "Step 0 Loss -0.208 Score 0.75\n",
      "Step 1 Loss -0.175 Score 0.75\n",
      "Step 2 Loss 0.043 Score 0.9\n",
      "Step 3 Loss 0.085 Score 0.9\n",
      "Step 4 Loss 0.246 Score 1.0\n",
      "Step 5 Loss 0.283 Score 1.0\n",
      "Step 6 Loss 0.314 Score 1.0\n",
      "Step 7 Loss 0.339 Score 1.0\n",
      "Step 8 Loss 0.359 Score 1.0\n",
      "Step 9 Loss 0.375 Score 1.0\n",
      "Step 10 Loss 0.388 Score 1.0\n",
      "Step 11 Loss 0.398 Score 1.0\n",
      "Step 12 Loss 0.406 Score 1.0\n",
      "Step 13 Loss 0.413 Score 1.0\n",
      "Step 14 Loss 0.419 Score 1.0\n",
      "Step 15 Loss 0.424 Score 1.0\n",
      "Step 16 Loss 0.428 Score 1.0\n",
      "Step 17 Loss 0.431 Score 1.0\n",
      "Step 18 Loss 0.494 Score 0.95\n",
      "Step 19 Loss 0.497 Score 0.95\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Procedure LMS\n",
    "1. Normalize data to have mean=0,std=1 for features\n",
    "2. Add interept term to data\n",
    "3. Initialize weights\n",
    "4. For step in n_iters do:\n",
    "5.     predict outputs - pred = sigmoid()\n",
    "6.     compute loss = mean( y log pred + (1-y) log (1-pred)) \n",
    "7.     compute grad_loss = mean((pred-targets)@data) \n",
    "8.     update weights -= lr*grad_loss (maximizes log likelihood)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_data(n,f):\n",
    "    data = np.random.random_sample((n,f))+np.sqrt(np.arange(n*f).reshape(n,f))\n",
    "    targets = np.concatenate((np.zeros(n//2),np.ones(n-n//2)))\n",
    "    return data,targets\n",
    "\n",
    "class LinearRegressionLMS:\n",
    "    \n",
    "    def __init__(self,lr=1e-2,iters=10):\n",
    "        self.lr = lr\n",
    "        self.iters = iters\n",
    "        self.weights = []\n",
    "        \n",
    "    def fit(self,data,targets):\n",
    "        print(self)\n",
    "        n = data.shape[0]\n",
    "        # normalize data + add intercept term\n",
    "        data -= data.mean(0)\n",
    "        data/=(data.std(0)+1e-5)\n",
    "        data = self._add_intercept(data)\n",
    "        f = data.shape[1]\n",
    "        # init and normalize weights\n",
    "        self.weights = np.random.randn(f)\n",
    "        self.weights = (self.weights- self.weights.mean())/(self.weights.std()+1e-5)\n",
    "        for i in range(self.iters):\n",
    "            # predict\n",
    "            pred = self._sigmoid(data @ self.weights)\n",
    "            # compute log probs for loss\n",
    "            logp0 = np.array([np.log(p) for p in pred if p >=.5])\n",
    "            logp1 = np.array([1-p for p in pred if p< 0.5])\n",
    "            # compute loss\n",
    "            loss = np.mean(np.concatenate((logp0,logp1)))\n",
    "            # compute gradient of loss\n",
    "            grad_loss = data.T @ (pred-targets) \n",
    "            # update weights\n",
    "            self.weights -= self.lr * grad_loss\n",
    "            score = self._score(pred,targets)\n",
    "            print('Step',i,'Loss',round(loss,3),'Score',score)\n",
    " \n",
    "    \n",
    "    def _score(self,pred,targets):\n",
    "        out = [0.0 if p>=0.5 else 1.0 for p in pred]\n",
    "        return np.mean([x==y for x,y in zip(out,targets)])\n",
    "\n",
    "    def _sigmoid(self,x):\n",
    "        return 1/(1+np.exp(x))\n",
    "    \n",
    "    def _grad_sigmoid(self,x):\n",
    "        return self._sigmoid(x)*(1-self._sigmoid(x))\n",
    "    \n",
    "    def _add_intercept(self,data):\n",
    "        intercept = np.ones(data.shape[0]).reshape(-1,1)\n",
    "        return np.concatenate((intercept,data),1)\n",
    "    \n",
    "    def __str__(self):\n",
    "        line = '='*40\n",
    "        print(line)\n",
    "        print('Logistic Regression')\n",
    "        print(line)\n",
    "        print('Hyperparamters:')\n",
    "        print('lr =',self.lr,'> learning rate')\n",
    "        print('iters =',self.iters,'> optimization steps')\n",
    "        return line\n",
    "    \n",
    "data, targets = generate_data(20,3)\n",
    "model = LinearRegressionLMS(lr=1e-2,iters=20)\n",
    "model.fit(data,targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
