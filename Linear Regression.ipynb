{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "We cover the LMS and Normal Equations methods for solving Linear Regression problems. Least Mean Squares (LMS) adjust the weights of the linear model using gradient descent, while the normal equations are an explicit solution for linear regression. \n",
    "\n",
    "The Normal Equations read \n",
    "\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "and an be derived explicitly by computing the gradient of the loss $\\nabla_\\theta J$ and finding its extremum where $\\nabla_\\theta J = 0$, where the loss is just that mean squared error between predictions and targets:\n",
    "\n",
    "$$J = \\frac 1 n \\sum_i^n (\\theta^T x - y)^2$$\n",
    "\n",
    "It is useful to check approximate solutions (such as LMS) by comparing them with the explicit result from the Normal Equations.\n",
    "\n",
    "This implementation of LMS also supports L2 regularization, which adds an additional term to the loss such that.\n",
    "\n",
    "$$J \\rightarrow J + \\alpha \\cdot \\theta^T \\theta $$\n",
    "\n",
    "Intuitively, the regularization term penalizes large values for $\\theta$ (since we're minimizing $J$) and $\\alpha \\geq 0 $ parameterizes the weight of the penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Linear Regression LMS\n",
      "========================================\n",
      "Hyperparamters:\n",
      "lr = 0.01 > learning rate\n",
      "alpha = 0.0 > regularization\n",
      "iters = 20 > optimization steps\n",
      "========================================\n",
      "Step 0 Loss 67.176\n",
      "Step 1 Loss 33.116\n",
      "Step 2 Loss 19.811\n",
      "Step 3 Loss 12.699\n",
      "Step 4 Loss 8.379\n",
      "Step 5 Loss 5.652\n",
      "Step 6 Loss 3.914\n",
      "Step 7 Loss 2.802\n",
      "Step 8 Loss 2.09\n",
      "Step 9 Loss 1.635\n",
      "Step 10 Loss 1.343\n",
      "Step 11 Loss 1.157\n",
      "Step 12 Loss 1.037\n",
      "Step 13 Loss 0.961\n",
      "Step 14 Loss 0.911\n",
      "Step 15 Loss 0.88\n",
      "Step 16 Loss 0.86\n",
      "Step 17 Loss 0.847\n",
      "Step 18 Loss 0.838\n",
      "Step 19 Loss 0.833\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Procedure LMS\n",
    "1. Normalize data to have mean=0,std=1 for features\n",
    "2. Add interept term to data\n",
    "3. Initialize weights\n",
    "4. For step in n_iters do:\n",
    "5.     predict outputs - pred\n",
    "6.     compute loss = .5*mean((pred-targets)**2)\n",
    "7.     compute grad_loss = mean((pred-targets)@data)\n",
    "8.     update weights -= lr*grad_loss\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_data(n,f):\n",
    "    data = np.random.random_sample((n,f))+np.sqrt(np.arange(n*f).reshape(n,f))\n",
    "    targets = np.arange(n)+np.random.randn(n)\n",
    "    return data,targets\n",
    "\n",
    "class LinearRegressionLMS:\n",
    "    \n",
    "    def __init__(self,lr=1e-2,iters=10,alpha=0.1):\n",
    "        self.lr = lr\n",
    "        self.iters = iters\n",
    "        self.alpha = alpha\n",
    "        self.weights = []\n",
    "        \n",
    "    def fit(self,data,targets):\n",
    "        print(self)\n",
    "        n = data.shape[0]\n",
    "        data -= data.mean(0)\n",
    "        data/=(data.std(0)+1e-5)\n",
    "        data = self._add_intercept(data)\n",
    "        f = data.shape[1]\n",
    "        self.weights = np.random.randn(f)\n",
    "        for i in range(self.iters):\n",
    "            pred = data @ self.weights\n",
    "            loss = .5*np.mean((pred-targets)**2)+ self.alpha*np.mean(self.weights)**2\n",
    "            grad_loss = data.T @ (pred-targets) + self.alpha*self.weights\n",
    "            self.weights -= self.lr * grad_loss\n",
    "            print('Step',i,'Loss',round(loss,3))\n",
    "        \n",
    "    def _add_intercept(self,data):\n",
    "        intercept = np.ones(data.shape[0]).reshape(-1,1)\n",
    "        return np.concatenate((intercept,data),1)\n",
    "    \n",
    "    def __str__(self):\n",
    "        line = '='*40\n",
    "        print(line)\n",
    "        print('Linear Regression LMS')\n",
    "        print(line)\n",
    "        print('Hyperparamters:')\n",
    "        print('lr =',self.lr,'> learning rate')\n",
    "        print('alpha =',self.alpha,'> regularization')\n",
    "        print('iters =',self.iters,'> optimization steps')\n",
    "        return line\n",
    "    \n",
    "data, targets = generate_data(20,3)\n",
    "model = LinearRegressionLMS(lr=1e-2,iters=20,alpha=0.0)\n",
    "model.fit(data,targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Linear Regression Normal Equations\n",
      "========================================\n",
      "Loss 2.1342620998443502\n"
     ]
    }
   ],
   "source": [
    "class LinearRegressionNormalEqs:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = []\n",
    "        \n",
    "    def fit(self,data,targets):\n",
    "        print(self)\n",
    "        n = data.shape[0]\n",
    "        data -= data.mean(0)\n",
    "        data/=(data.std(0)+1e-5)\n",
    "        data = self._add_intercept(data)\n",
    "        f = data.shape[1]\n",
    "        self.weights = np.linalg.inv(data.T @ data) @ (data.T @ targets)\n",
    "        pred = data @ self.weights\n",
    "        loss = .5*np.mean((pred-targets)**2)\n",
    "        print('Loss',loss)\n",
    "        \n",
    "    def _add_intercept(self,data):\n",
    "        intercept = np.ones(data.shape[0]).reshape(-1,1)\n",
    "        return np.concatenate((intercept,data),1)\n",
    "    \n",
    "    def __str__(self):\n",
    "        line = '='*40\n",
    "        print(line)\n",
    "        print('Linear Regression Normal Equations')\n",
    "        return line\n",
    "    \n",
    "data, targets = generate_data(20,3)\n",
    "model = LinearRegressionNormalEqs()\n",
    "model.fit(data,targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
